{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing Assignment 1**"
      ],
      "metadata": {
        "id": "7s63z2d2d5ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"szhuggingface/ag_news\")\n",
        "\n",
        "#cite\n",
        "'''\n",
        "@inproceedings{zhang2015character,\n",
        "  title={Character-level convolutional networks for text classification},\n",
        "  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n",
        "  booktitle={Advances in neural information processing systems},\n",
        "  pages={649--657},\n",
        "  year={2015}\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "69vLpJSPbVAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading AG News dataset...\")\n",
        "ds = load_dataset(\"ag_news\")\n",
        "print(\"Dataset loaded.\")\n",
        "\n",
        "print(\"\\nOriginal dataset structure:\")\n",
        "print(ds)\n",
        "print(\"\\nOriginal sample text:\")\n",
        "print(ds['train'][0]['text'])"
      ],
      "metadata": {
        "id": "y8CTNxbJflcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Pre-processing**"
      ],
      "metadata": {
        "id": "hYl5juKIhdAz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ0YYIfpYEkI"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 1) **Lowercasing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFC1axshYEkJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def spacy_process_batch(batch):\n",
        "    processed_texts = []\n",
        "    for doc in nlp.pipe(batch['text']):\n",
        "        lower_tokens = [token.lower_ for token in doc]\n",
        "        processed_texts.append(\" \".join(lower_tokens))\n",
        "\n",
        "    batch['text'] = processed_texts\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lowercased_ds = ds.map(spacy_process_batch, batched=True)\n",
        "print(\"Processing complete.\")"
      ],
      "metadata": {
        "id": "glQ2s93Ug0B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- ORIGINAL TEXT (Sample 0) ---\")\n",
        "print(ds['train'][0]['text'])\n",
        "\n",
        "print(\"\\n--- PROCESSED TEXT (Sample 0) ---\")\n",
        "print(lowercased_ds['train'][0]['text'])\n",
        "\n",
        "print(\"\\n--- ORIGINAL TEXT (Sample 1) ---\")\n",
        "print(ds['train'][1]['text'])\n",
        "\n",
        "print(\"\\n--- PROCESSED TEXT (Sample 1) ---\")\n",
        "print(lowercased_ds['train'][1]['text'])"
      ],
      "metadata": {
        "id": "f5ijXoxkg7Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQs5uUrSYEkR"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 2) **Removal of URLs**\n",
        "\n",
        "Removing them first as later on with removal of special characters the structure to identify a URL would be destroyed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "\n",
        "def remove_urls_batch(batch):\n",
        "    batch['text'] = [url_pattern.sub(r'', text) for text in batch['text']]\n",
        "    return batch"
      ],
      "metadata": {
        "id": "UWk9mSeQocQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nourl_ds = lowercased_ds.map(remove_urls_batch, batched=True)\n",
        "print(\"Processing complete. New dataset is 'nourl_ds'.\")"
      ],
      "metadata": {
        "id": "0APzH25Eps7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = -1\n",
        "print(\"\\nSearching for a sample with a URL...\")\n",
        "for i, doc in enumerate(ds['train']):\n",
        "    if url_pattern.search(doc['text']):\n",
        "        sample_index = i\n",
        "        print(f\"Found sample with URL at index: {sample_index}\")\n",
        "        break\n",
        "\n",
        "if sample_index == -1:\n",
        "    print(\"No URL found, using index 0 as a fallback.\")\n",
        "    sample_index = 0\n",
        "print(f\"--- Showing results for sample index: {sample_index} ---\")\n",
        "\n",
        "print(\"\\n--- BEFORE URL REMOVAL (from 'lowercased_ds') ---\")\n",
        "# Get the text from the 'lowercased_ds' (it's already lowercased)\n",
        "print(lowercased_ds['train'][sample_index]['text'])\n",
        "\n",
        "print(\"\\n--- AFTER URL REMOVAL (from 'nourl_ds') ---\")\n",
        "# Get the text from our new 'nourl_ds'\n",
        "print(nourl_ds['train'][sample_index]['text'])"
      ],
      "metadata": {
        "id": "nODRRM-Pptte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) **Remove HTML tags**"
      ],
      "metadata": {
        "id": "Mwee1IX2qhkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html_pattern = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_html_batch(batch):\n",
        "    batch['text'] = [html_pattern.sub(r' ', text) for text in batch['text']]\n",
        "    return batch\n"
      ],
      "metadata": {
        "id": "vbIdipzlqg2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nohtml_ds = nourl_ds.map(remove_html_batch, batched=True)\n",
        "\n",
        "print(\"Processing complete. New dataset is 'nohtml_ds'.\")"
      ],
      "metadata": {
        "id": "r-l9UVzMq5kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = -1\n",
        "print(\"Searching for a sample with HTML tags in 'nourl_ds'...\")\n",
        "\n",
        "for i, doc in enumerate(nourl_ds['train']):\n",
        "    # We use our compiled pattern to search the text\n",
        "    if html_pattern.search(doc['text']):\n",
        "        sample_index = i\n",
        "        print(f\"Found sample with HTML at index: {sample_index}\")\n",
        "        break\n",
        "\n",
        "if sample_index == -1:\n",
        "    print(\"No HTML tags found in the first 5000 samples. Using index 4 as a known example.\")\n",
        "    # The AG News dataset has known HTML in sample 4\n",
        "    sample_index = 4\n",
        "print(f\"--- Showing results for sample index: {sample_index} ---\")\n",
        "\n",
        "print(\"\\n--- BEFORE HTML REMOVAL (from 'nourl_ds') ---\")\n",
        "# Get the text from the 'nourl_ds'\n",
        "print(nourl_ds['train'][sample_index]['text'])\n",
        "\n",
        "print(\"\\n--- AFTER HTML REMOVAL (from 'nohtml_ds') ---\")\n",
        "# Get the text from our new 'nohtml_ds'\n",
        "print(nohtml_ds['train'][sample_index]['text'])"
      ],
      "metadata": {
        "id": "MzfoVOBvq56u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stzcyO6UYEkN"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 4) **Removing Punctuation & Special Characters**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWhA4M2qYEkO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_punctuation_batch(batch):\n",
        "    processed_texts = []\n",
        "    pattern = r'[^a-zA-Z]'\n",
        "\n",
        "    for text in batch['text']:\n",
        "        processed_text = re.sub(pattern, ' ', text)\n",
        "        processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
        "        processed_texts.append(processed_text)\n",
        "\n",
        "    batch['text'] = processed_texts\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nopunc_ds = nohtml_ds.map(remove_punctuation_batch, batched=True)\n",
        "print(\"Processing complete.\")"
      ],
      "metadata": {
        "id": "CyWey8uligqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- ORIGINAL TEXT (Sample 0) ---\")\n",
        "print(lowercased_ds['train'][0]['text'])\n",
        "\n",
        "print(\"\\n--- PROCESSED TEXT (Sample 0) ---\")\n",
        "print(nopunc_ds['train'][0]['text'])\n",
        "\n",
        "print(\"\\n--- ORIGINAL TEXT (Sample 1) ---\")\n",
        "print(lowercased_ds['train'][1]['text'])\n",
        "\n",
        "print(\"\\n--- PROCESSED TEXT (Sample 1) ---\")\n",
        "print(nopunc_ds['train'][1]['text'])"
      ],
      "metadata": {
        "id": "iMDUMtfxinsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4as5p2LSYEkP"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 5) **Stop - Words Removal**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2mLo9WFYEkQ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words_set = set(stop_words)\n",
        "print(f\"Loaded {len(stop_words_set)} English stop words (customized).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_batch(batch):\n",
        "    processed_texts = []\n",
        "\n",
        "    for text in batch['text']:\n",
        "        word_tokens = text.split()\n",
        "        filtered_words = [word for word in word_tokens if word not in stop_words_set]\n",
        "        final_text = \" \".join(filtered_words)\n",
        "        processed_texts.append(final_text)\n",
        "\n",
        "    batch['text'] = processed_texts\n",
        "    return batch"
      ],
      "metadata": {
        "id": "d9qyEYDLktHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nostop_ds = nopunc_ds.map(remove_stopwords_batch, batched=True)\n",
        "print(\"Processing complete.\")"
      ],
      "metadata": {
        "id": "VhGcwIXak5nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = -1\n",
        "for i, doc in enumerate(nopunc_ds['train']):\n",
        "    words = doc['text'].split()\n",
        "    # Check if any word in the document is in our stop word set\n",
        "    if any(word in stop_words_set for word in words):\n",
        "        sample_index = i\n",
        "        break # We found one, so we stop looking\n",
        "if sample_index != -1:\n",
        "    print(f\"Found a good sample at index: {sample_index}\")\n",
        "\n",
        "    print(\"\\nBEFORE STOP-WORDS (from nopunc_ds)\")\n",
        "    # Get the \"before\" text\n",
        "    before_text = nopunc_ds['train'][sample_index]['text']\n",
        "    print(before_text)\n",
        "\n",
        "    print(\"\\nAFTER STOP-WORDS (from nostop_ds)\")\n",
        "    # Get the \"after\" text for the *same index*\n",
        "    after_text = nostop_ds['train'][sample_index]['text']\n",
        "    print(after_text)\n",
        "\n",
        "    # Highlight the removed words\n",
        "    before_words = set(before_text.split())\n",
        "    after_words = set(after_text.split())\n",
        "    removed_words = before_words.difference(after_words)\n",
        "\n",
        "    print(\"\\nWORDS REMOVED\")\n",
        "    print(removed_words)\n",
        "\n",
        "else:\n",
        "    print(\"Could not find a sample with stop words.\")\n",
        "\n",
        "start_search_index = sample_index + 1\n",
        "second_sample_index = -1\n",
        "\n",
        "print(f\"Searching for next sample starting from index {start_search_index}...\")\n",
        "\n",
        "# Iterate from the next index to the end of the training set\n",
        "for i in range(start_search_index, len(nopunc_ds['train'])):\n",
        "    doc_text = nopunc_ds['train'][i]['text']\n",
        "    words = doc_text.split()\n",
        "\n",
        "    # Check if any word in this document is a stop word\n",
        "    if any(word in stop_words_set for word in words):\n",
        "        second_sample_index = i  # Found the next one!\n",
        "        break # Stop the loop\n",
        "\n",
        "# Display the results for the second sample\n",
        "if second_sample_index != -1:\n",
        "    print(f\"\\nFound a second sample at index: {second_sample_index}\")\n",
        "\n",
        "    print(\"\\nBEFORE STOP-WORDS (from nopunc_ds)\")\n",
        "    before_text = nopunc_ds['train'][second_sample_index]['text']\n",
        "    print(before_text)\n",
        "\n",
        "    print(\"\\nAFTER STOP-WORDS (from nostop_ds)\")\n",
        "    after_text = nostop_ds['train'][second_sample_index]['text']\n",
        "    print(after_text)\n",
        "\n",
        "    # Highlight the removed words\n",
        "    before_words = set(before_text.split())\n",
        "    after_words = set(after_text.split())\n",
        "    removed_words = before_words.difference(after_words)\n",
        "\n",
        "    print(\"\\nWORDS REMOVED\")\n",
        "    print(removed_words)\n",
        "\n",
        "else:\n",
        "    print(\"Could not find another sample with stop words.\")"
      ],
      "metadata": {
        "id": "Z2WlwSlclj_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKhwe5BiYEkc"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 6) **Stemming**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCeERUIpYEke"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_batch(batch):\n",
        "  processed_texts = []\n",
        "  for text in batch['text']:\n",
        "        word_tokens = text.split()\n",
        "        stemmed_words = [stemmer.stem(word) for word in word_tokens]\n",
        "        final_text = \" \".join(stemmed_words)\n",
        "        processed_texts.append(final_text)\n",
        "\n",
        "  batch['text'] = processed_texts\n",
        "  return batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_ds = nostop_ds.map(stem_batch, batched=True)\n",
        "print(\"Processing complete. New dataset is 'stemmed_ds'.\")"
      ],
      "metadata": {
        "id": "Rg8__dRYzctz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = -1\n",
        "\n",
        "print(\"Searching for a sample affected by stemming...\")\n",
        "\n",
        "# We loop until we find a text that was changed\n",
        "for i in range(len(nostop_ds['train'])):\n",
        "    before_text = nostop_ds['train'][i]['text']\n",
        "    after_text = stemmed_ds['train'][i]['text']\n",
        "\n",
        "    if before_text != after_text:\n",
        "        sample_index = i  # We found one!\n",
        "        break # Stop the loop\n",
        "\n",
        "# Now we display the sample we found\n",
        "if sample_index != -1:\n",
        "    print(f\"--- Found a good sample at index: {sample_index} ---\")\n",
        "\n",
        "    print(\"\\n--- BEFORE STEMMING (from nostop_ds) ---\")\n",
        "    before_text = nostop_ds['train'][sample_index]['text']\n",
        "    print(before_text)\n",
        "\n",
        "    print(\"\\n--- AFTER STEMMING (from stemmed_ds) ---\")\n",
        "    after_text = stemmed_ds['train'][sample_index]['text']\n",
        "    print(after_text)\n",
        "\n",
        "    # Highlight the changed words\n",
        "    before_words = before_text.split()\n",
        "    after_words = after_text.split()\n",
        "\n",
        "    print(\"\\n--- WORDS CHANGED ---\")\n",
        "    changed_words = []\n",
        "    for b, a in zip(before_words, after_words):\n",
        "        if b != a:\n",
        "            changed_words.append(f\"'{b}' -> '{a}'\")\n",
        "\n",
        "    print(\", \".join(changed_words))\n",
        "\n",
        "else:\n",
        "    print(\"Could not find a sample that was changed by stemming.\")"
      ],
      "metadata": {
        "id": "0tZ2pTd-0xCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A3pCt0TYEkf"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 7) **Lemmatization**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0r0iwa4YEkh"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        # Default to noun if no match\n",
        "        return wordnet.NOUN"
      ],
      "metadata": {
        "id": "AqztH45-5gIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_with_pos_batch(batch):\n",
        "   processed_texts = []\n",
        "\n",
        "   for text in batch['text']:\n",
        "        word_tokens = text.split()\n",
        "        pos_tagged_tokens = nltk.pos_tag(word_tokens)\n",
        "        lemmatized_words = []\n",
        "        for word, tag in pos_tagged_tokens:\n",
        "          wordnet_tag = get_wordnet_pos(tag)\n",
        "          lemma = lemmatizer.lemmatize(word, pos=wordnet_tag)\n",
        "          lemmatized_words.append(lemma)\n",
        "        final_text = \" \".join(lemmatized_words)\n",
        "        processed_texts.append(final_text)\n",
        "   batch['text'] = processed_texts\n",
        "   return batch"
      ],
      "metadata": {
        "id": "jeusG-zJ4WBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_ds = nostop_ds.map(lemmatize_with_pos_batch, batched=True)\n",
        "\n",
        "print(\"Processing complete. New dataset is 'lemmatized_ds_pos'.\")"
      ],
      "metadata": {
        "id": "VHJFzJ5T4qD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(f\"--- Showing results for sample index: {sample_index} ---\")\n",
        "except NameError:\n",
        "    print(\"Could not find 'sample_index', using index 1 instead.\")\n",
        "    sample_index = 1 # Fallback to a known good sample\n",
        "\n",
        "print(\"\\n--- ORIGINAL (from nostop_ds) ---\")\n",
        "print(nostop_ds['train'][sample_index]['text'])\n",
        "\n",
        "print(\"\\n--- STEMMED (from stemmed_ds) ---\")\n",
        "print(stemmed_ds['train'][sample_index]['text'])\n",
        "\n",
        "print(\"\\n--- LEMMATIZED (from lemmatized_ds) ---\")\n",
        "print(lemmatized_ds['train'][sample_index]['text'])\n"
      ],
      "metadata": {
        "id": "V5gi8zC_4zmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT1ANehtYEki"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 8) **Tokenization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import itertools"
      ],
      "metadata": {
        "id": "bdDJhfKG8sXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzsPdvcsYEki",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "print(\"Preparing final corpus for tokenization...\")\n",
        "train_texts = list(lemmatized_ds['train']['text'])\n",
        "test_texts = list(lemmatized_ds['test']['text'])\n",
        "corpus = train_texts + test_texts\n",
        "print(f\"Corpus prepared with {len(corpus)} total documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 20000 # Top 20,000 words\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "print(\"Fitting tokenizer...\")\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "print(\"Fitting complete.\")"
      ],
      "metadata": {
        "id": "tpG0Sudk9icC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(\"\\n--- Word Index Sample (Top 10) ---\")\n",
        "print(dict(itertools.islice(word_index.items(), 10)))"
      ],
      "metadata": {
        "id": "H3ZQJzP79oXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Final 'Text-to-Sequence' Result ---\")\n",
        "sample_text = corpus[1]\n",
        "sequence = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "print(f\"ORIGINAL TEXT:\\n{sample_text}\")\n",
        "print(f\"\\nNUMERICAL SEQUENCE:\\n{sequence[0]}\")"
      ],
      "metadata": {
        "id": "3t1zssrt9wM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Processing for Natural Language Processing (NLP)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IqHME5fIaHW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 1) **Bag of Words (BoW)**\n",
        "\n"
      ],
      "metadata": {
        "id": "ez22mVV0btEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Preparing the final labels...\")\n",
        "train_labels = list(lemmatized_ds['train']['label'])\n",
        "test_labels = list(lemmatized_ds['test']['label'])\n",
        "labels = train_labels + test_labels\n",
        "#corpus prepared in tokenization\n",
        "print(f\"Corpus prepared with {len(corpus)} documents.\")\n",
        "print(f\"Labels prepared with {len(labels)} labels.\")"
      ],
      "metadata": {
        "id": "paLcyIBocDbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(min_df=3)\n",
        "print(\"Fitting Bag of Words model (CountVectorizer)...\")\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Bag of Words model fitted.\")\n",
        "print(\"\\n--- Shape of the Feature Matrix (X) ---\")\n",
        "print(f\"(documents, features): {X.shape}\")\n",
        "print(f\"This means: {X.shape[0]} documents and {X.shape[1]} unique words (features).\")\n",
        "\n",
        "print(\"\\n--- Sample of Vocabulary (features) ---\")\n",
        "print(feature_names[5000:5010])"
      ],
      "metadata": {
        "id": "VAAjxcJzcGzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing results in tabular form\n",
        "X_array_small = X[0:5].toarray()\n",
        "index_names = [f\"Doc_{i}\" for i in range(5)]\n",
        "df = pd.DataFrame(data=X_array_small, columns=feature_names, index=index_names)\n",
        "print(\"--- Bag of Words Matrix (First 5 Documents) ---\")\n",
        "# Display the DataFrame. It will be very wide!\n",
        "# We can't display all 20,000+ columns, but here's a sample:\n",
        "df.iloc[:, 2000:2010]"
      ],
      "metadata": {
        "id": "AJWJ6AvGcIv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 2) **Term Frequency-Inverse Document Frequency (TF-IDF)**\n",
        "\n"
      ],
      "metadata": {
        "id": "c4IZsAZecK5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_ds = lemmatized_ds\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "print(f\"Corpus prepared with {len(corpus)} documents.\")"
      ],
      "metadata": {
        "id": "00yPwWw7cLmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud -q"
      ],
      "metadata": {
        "id": "z3l753mJHA6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "AecOp-HCHEXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "print(\"Fitting TF-IDF model...\")\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"TF-IDF model fitted.\")\n",
        "print(f\"Shape of TF-IDF Matrix: {X_tfidf.shape}\")"
      ],
      "metadata": {
        "id": "R1bhgqHFH9_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating Word Cloud... ☁️\")\n",
        "total_tfidf_scores = X_tfidf.sum(axis=0).A1\n",
        "tfidf_freqs = dict(zip(feature_names, total_tfidf_scores))\n",
        "wordcloud = WordCloud(width=800,\n",
        "                      height=400,\n",
        "                      background_color='white').generate_from_frequencies(tfidf_freqs)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off') # Hide the x and y axes\n",
        "plt.title(\"TF-IDF Word Cloud for AG News Dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wl5p0Jt2HjCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advaced Level (Optional for Basic Level)**"
      ],
      "metadata": {
        "id": "VmVo9iL3dVyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 3) **Encodings**\n"
      ],
      "metadata": {
        "id": "8c-59GzxdYSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Preparing the final corpus...\")\n",
        "\n",
        "\n",
        "train_texts = list(final_ds['train']['text'])\n",
        "test_texts = list(final_ds['test']['text'])\n",
        "corpus = train_texts + test_texts\n",
        "\n",
        "print(f\"Corpus prepared with {len(corpus)} documents.\")"
      ],
      "metadata": {
        "id": "b-4qIoB_daUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vectorizer = CountVectorizer(min_df=3)\n",
        "X_bow = bow_vectorizer.fit_transform(corpus)\n",
        "ohe_vectorizer = CountVectorizer(min_df=3, binary=True)\n",
        "X_ohe = ohe_vectorizer.fit_transform(corpus)\n",
        "feature_names = ohe_vectorizer.get_feature_names_out()\n",
        "print(\"Bag of Words (BoW) and One-Hot (OHE) models fitted.\")\n",
        "print(f\"\\nShape of both matrices is identical: {X_ohe.shape}\")"
      ],
      "metadata": {
        "id": "5G4RBRP3KskC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 4) **Word Embeddings**\n",
        "\n",
        "Word embeddings are <blue>**dense vector representations**</blue> of words that capture their meanings by placing words with <blue>**similar meanings closer**</blue> in <blue>**vector space**</blue>. Unlike BoW or one-hot encoding, embeddings <blue>**capture relationships**</blue> between words based on context. For example, in an embedding space, words like \"king\" and \"queen\" or \"apple\" and \"fruit\" would be closer together, reflecting their <blue>**semantic similarity**</blue>. These embeddings are learned from large datasets and can be used as inputs to machine learning models for various NLP tasks. Popular techniques for generating word embeddings include <blue>**Word2Vec**</blue>, <blue>**GloVe**</blue>, and <blue>**FastText**</blue>."
      ],
      "metadata": {
        "id": "XPA4J8ZKdel9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 5) **Word2Vec**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x_SAwrsCdhL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim -q"
      ],
      "metadata": {
        "id": "LImIQuPkLOie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "print(f\"Original corpus has {len(corpus)} documents.\")\n",
        "print(\"\\nOriginal document (string)\")\n",
        "print(corpus[0])\n",
        "\n",
        "\n",
        "sentences = [doc.split() for doc in corpus]\n",
        "\n",
        "print(\"\\nPrepared 'sentence' (list of tokens)\")\n",
        "print(sentences[0])\n",
        "print(f\"\\nData is now a list of {len(sentences)} lists, ready for Word2Vec.\")"
      ],
      "metadata": {
        "id": "mx-DcQ5nLRga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training CBOW model (sg=0)...\")\n",
        "# sg=0 means CBOW\n",
        "cbow_model = Word2Vec(sentences,\n",
        "                      vector_size=100,\n",
        "                      window=5,\n",
        "                      min_count=3,\n",
        "                      sg=0) # 0 for CBOW\n",
        "\n",
        "print(\"Training Skip-gram model (sg=1)...\")\n",
        "# sg=1 means Skip-gram\n",
        "skipgram_model = Word2Vec(sentences,\n",
        "                          vector_size=100,\n",
        "                          window=5,\n",
        "                          min_count=3,\n",
        "                          sg=1) # 1 for Skip-gram\n",
        "\n",
        "print(\"Models trained successfully.\")"
      ],
      "metadata": {
        "id": "7Z8EYtd4djlX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}